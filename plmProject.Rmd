---
title: "Machine Learning Project"
author: "Greg E"
date: "January 22, 2017"
output: html_document
---

# Setup
```{r, Chunk0}
set.seed(777)
library(caret)
library(rpart)
library(rattle)
library(rpart.plot)
library(ggplot2)
library(randomForest)
```

# Load the Data.
I pull it down once and cache this chunk, so that we don't need to pull the data every time we run.  To pull data to another computer change cache to FALSE. 
```{r, Chunk1, cache=TRUE} 
pmltraining <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))
pmltesting <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))
dim(pmltraining)
dim(pmltesting)
```

# Cleaning and Preprocessing the data.
You first must preprocess the data.  Models do not handle NA predictors so you need to impute these or delete them. This may be the trickiest part of the process.  There are a lot of variables in the data, and most will not be good predictors.

### Start by removing the nulls and NAs.
```{r, Chunk2a, cache=TRUE}
cleanTrain <- pmltraining[, colSums(is.na(pmltraining)) == 0]
dim(cleanTrain)
```

### Next eliminate the near Zero Variance variables.
```{r, Chunk2b, cache=TRUE}
nzv <- nearZeroVar(cleanTrain, saveMetrics = TRUE)
cleanTrain2 <- cleanTrain[,nzv$nzv==FALSE]
dim(cleanTrain2)
```

### Also remove descriptive fields that should not confuse the modeling.
```{r, Chunk2c, cache=TRUE}
cleanTrain2 <- cleanTrain2[, -c(1:7)]
dim(cleanTrain2)
```

## We now splice the cleaned data, to create training and testing data sets.   
```{r, Chunk3, cache=TRUE}
inTrain <- createDataPartition(y=cleanTrain2$classe, p=0.7, list=FALSE)
training <- cleanTrain2[inTrain,]
testing <- cleanTrain2[-inTrain,]
dim(training)
dim(testing)
```

## How I built the model.
I tried several methods in order to compare accuracy.  Attempts were done with "lm", "glm", "rpart", and "rf".  I also tried to use preprocessing with Principal Component Analysis (PCA) to ignore highly correltated variables to further reduce variable processing.

### The linear models did not work with this many predictors.
```{r, Chunk4a, cache=TRUE}
# modFit <- train(classe ~ ., method="glm", preProcess="pca", data=training)
# print(modFit$finalModel)
```

### I tried predicting with trees.  Rpart worked but the accuracy was very low.
```{r, Chunk4b, cache=TRUE}
modFitRpart <- train(classe ~ ., method="rpart", preProcess="pca", data=training)
print(modFitRpart)
fancyRpartPlot(modFitRpart$finalModel)
```

Accuracy is 43%, which is better than the 20% you'd have for a 1 in 5 chance of correctly guessing the correct classe.  But not great.

### Random Forests worked well, but was very slow.  Don't run this unless you have several hours!
```{r, Chunk4c, cache=TRUE}
modFitRF <- train(classe ~ ., method="rf", data=training, prox=TRUE)
```

## How cross Validation was done.
Forest and trees creates multiple trees from bootstrap samples for all predictors, and then uses algorithms to compute the most accurate predictors.  The model shows incredible accuracy on the Training set.   
```{r, Chunk5}
print(modFitRF)
```

## Show correct outcomes and sample errors when run on the Testing set.    
Pedicting new Values.
```{r, Chunk 6}
pred <- predict(modFitRF,testing)
testing$predRight <-pred==testing$classe
table(pred,testing$classe)
```

The confusion matrix table of Correct predictions, using our Random Forest model on the testing data, set shows 5843 correct predictions versus 42 false predictions, which is a 99.28% accuracy.  

## Why I made decisions I did. 
There are too many variables to attempt a Linear Model, and an attempt to do so with the "lm" and "glm" methods to train in the Caret package did not even work.  

Predicting with Trees using Rpart is a more advanced method with classification trace.  It was extremely fast.  Unfortunatly the accuracy was poor as the final model print showed.   

Random Forest is known to be a very accurate tool, with top performing algorithms, but it is also known to be slow.  I was very happy with the very high accuracy for the large number of predictions (52) predictors, though I had to leave this running overnight to get the final model built.  

From an accuracy point of view, Random Forest was the best choice.     